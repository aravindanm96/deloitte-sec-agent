# Code Generated by Sidekick is for learning and experimentation purposes only.

import streamlit as st

# ---------- UI STYLING ----------
st.markdown(
    """
    <style>
    .rounded-title {
        background-color: #111;
        color: #fff;
        padding: 0 28px;
        height: 72px;
        display: flex;
        align-items: center;
        border-bottom: 2px solid #222;
        box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        margin-bottom: 0px;
        border-radius: 18px;
    }
    .rounded-logo {
        background-color: #111;
        display: flex;
        align-items: center;
        justify-content: center;
        height: 72px;
        border-radius: 18px;
        margin-right: 8px;
    }
    .rounded-logo img {
        border-radius: 12px;
        background: #222;
        padding: 4px;
    }
    </style>
    """,
    unsafe_allow_html=True
)

col_logo, col_title = st.columns([1, 8])
with col_logo:
    st.image("images/Deloitte.png", width=85)
with col_title:
    st.markdown("""
        <div style="background-color: #111; color: #fff; padding: 0 20px; height: 52px; display: flex; align-items: center; border-bottom: 2px solid #222; box-shadow: 0 2px 6px rgba(0,0,0,0.08); margin-bottom: 0px; border-radius: 18px;">
            <h1 style="color: #fff; margin: 2; font-size: 2rem; font-family: 'Segoe UI', Arial, sans-serif;">
                SEC Filing Chatbot
            </h1>
        </div>
    """, unsafe_allow_html=True)

st.markdown("<br><br>", unsafe_allow_html=True)

import os
import numpy as np
from langchain.document_loaders import (
    CSVLoader, UnstructuredPDFLoader, UnstructuredExcelLoader, TextLoader, PyPDFLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import BedrockEmbeddings
from langchain.vectorstores import FAISS
import boto3
from config import *
AWS_REGION = os.getenv("AWS_REGION")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")

# ---------- CONFIG & CONSTANTS ----------
os.makedirs(INDEXES_DIR, exist_ok=True)
current_dir = os.getcwd()


bedrock_client = boto3.client(
    service_name="bedrock-runtime",
    region_name=AWS_REGION,
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)
embedding_model = BedrockEmbeddings(client=bedrock_client, model_id="amazon.titan-embed-text-v2:0")

# ---------- UTILITY FUNCTIONS ----------
def sanitize_index_name(name):
    """Sanitize index name: replace non-alphanumeric characters with underscores and convert to lowercase."""
    return "".join(c if c.isalnum() else "_" for c in name).lower()

def create_index_dir(indexes_dir, index_name):
    """Create the index directory if it doesn't exist."""
    path = os.path.join(indexes_dir, index_name)
    if not os.path.exists(path):
        os.makedirs(path)
    return path

def load_document(file):
    name = file.name.lower()
    if name.endswith(".csv"):
        return CSVLoader(file).load()
    elif name.endswith((".xlsx", ".xls")):
        return UnstructuredExcelLoader(file).load()
    elif name.endswith(".pdf"):
        full_path = os.path.join(current_dir, subfolder, file.name)
        return PyPDFLoader(full_path).load()
    else:
        return TextLoader(file).load()

def auto_chunk_params(documents):
    """
    Dynamically determine chunk size and overlap based on document statistics.
    - Uses median and variance of document lengths.
    - Adjusts for file type (PDFs tend to need larger chunks).
    """
    if not documents:
        return 1000, 150  # sensible defaults

    lengths = np.array([len(doc.page_content) for doc in documents])
    median_length = int(np.median(lengths))
    max_length = int(np.max(lengths))
    min_length = int(np.min(lengths))
    std_length = int(np.std(lengths))

    # Default chunking
    chunk_size = 1000
    chunk_overlap = 150

    # Adapt chunk size based on median and variance
    if median_length < 500:
        chunk_size = 400
        chunk_overlap = 50
    elif median_length < 1200:
        chunk_size = 800
        chunk_overlap = 100
    elif median_length < 2500:
        chunk_size = 1200
        chunk_overlap = 200
    else:
        chunk_size = 1800
        chunk_overlap = 300

    # If high variance, use smaller chunks to avoid splitting context
    if std_length > 2000:
        chunk_size = max(600, chunk_size // 2)
        chunk_overlap = max(80, chunk_overlap // 2)

    # If any PDFs, use larger chunks
    if any(getattr(doc, "metadata", {}).get("source", "").endswith(".pdf") for doc in documents):
        chunk_size = max(chunk_size, 1200)
        chunk_overlap = max(chunk_overlap, 200)

    # Bound chunk size and overlap
    chunk_size = max(200, min(chunk_size, 3000))
    chunk_overlap = max(20, min(chunk_overlap, chunk_size // 2))

    return chunk_size, chunk_overlap

# ---------- MAIN WORKFLOW ----------
st.markdown("**Please enter a unique index name before starting the process.**")

index_name = st.text_input("Index Name", value="", key="index_name")
uploaded_files = st.file_uploader("Upload SEC Q10/Q4 files", accept_multiple_files=True)
start_process = st.button("Start", type="primary", key="start_btn")

if start_process:
    if not index_name.strip():
        st.warning("Index name cannot be empty. Please enter a name before starting.")
        st.stop()
    if not uploaded_files or len(uploaded_files) == 0:
        st.warning("Please upload at least one file before starting the process.")
        st.stop()

    index_name = sanitize_index_name(index_name)
    index_path = create_index_dir(INDEXES_DIR, index_name)

    with st.spinner("Building FAISS index. This may take a few moments..."):
        docs = []
        for f in uploaded_files:
            docs.extend(load_document(f))

        chunk_size, chunk_overlap = auto_chunk_params(docs)
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        split_docs = splitter.split_documents(docs)

        persist_path = os.path.join(INDEXES_DIR, index_name)
        try:
            faiss_db = FAISS.from_documents(split_docs, embedding_model)
            faiss_db.save_local(persist_path)
            st.success(
                f"✅ FAISS index '{index_name}' successfully built and stored at {persist_path} "
                f"(chunk_size={chunk_size}, overlap={chunk_overlap})"
            )
        except Exception as e:
            st.error(f"❌ Failed to build FAISS index: {e}")

else:
    st.info("Ready to start. Enter an index name, upload files, then click 'Start'.")
